# **Behavioral Cloning** 

---

In this project the deep learning model is implemented to drive the car autonomously on the simulator. The goals/steps of this project are the following:

* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report


[//]: # (Image References)

[image1]: ./Write_Up/Model.png "Model Visualization"
[image2]: ./Write_Up/croppedImg.png "CroppedImage"
[image3]: ./Write_Up/flippedImage.png "Original and flip"
[image4]: ./Write_Up/data_before.png "data visulaization"
[image5]: ./Write_Up/epoch_graph.png "Error Loss"

## Rubric Points
---
### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:
* model.py containing the script to create and train the model
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network 
* writeup_p4_behavioralCloning.md  summarizing the results
* video_images.mpd showing the result
* video_images file including the images taken from simulator running the model.h5
* WriteUp files including all images used in WriteUp

#### 2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing 
```sh
python drive.py model.h5
```
As a result, the packed video images generated from the simulator using autonmous option are also provided under the files `video_images` in my workspace. The video is also generated by executing 
```sh
python video.py video_images
```
which results in the `video_images.mpg` that is alsp provided in the workspace.

#### 3. Submission code is usable and readable

The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.

### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed

I used first of all basic LeNet model, but the result was very strange and bad. Therefore I have decided to use NVIDIA model which is proposed in the course. 

My model consists of a convolution neural network with 3x3 (depths: 64 ) and 5x5 (depths: 24, 36 and 48 )filter sizes (model.py lines 82-103) 

The model uses elu as a activation function, and the data is normalized in the model using a Keras lambda layer (code line 84), and also cropped using Keras cropping layer (code line 86). 

#### 2. Attempts to reduce overfitting in the model

The model contains pooling with pool window size 2x2 and dropout layer with keeping probability 0.5 in order to reduce overfitting (model.py lines 95 and 97). 

The model was trained and validated on different data sets to ensure that the model was not overfitting. The training data and validation data are splitted. %80 for training data and %20 for validation data are chosen. The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track. The result is provided in the video_image.mp4 in my workspace

#### 3. Model parameter tuning

The model used an ADAM optimizer, so the learning rate was not tuned manually (model.py line 111).
The MSE is used as loss indicator to check if the error values stays closer to 0 as much as possible.

#### 4. Appropriate training data

Training data was chosen to keep the vehicle driving on the road. I have used the data provide by Udacity which has a combination of center lane driving, recovering from the left and right sides of the road. I used each image and also generated augmented dat to train the model

For details about how I created the training data, see the next section. 

### Model Architecture and Training Strategy

#### 1. Solution Design Approach

The overall strategy for deriving a model architecture was to try and change the model to reach the best result.

In order to gauge how well the model was working, I split my image and steering angle data into a training and validation set. I found that my first model had a low mean squared error on the training set but a high mean squared error on the validation set. This implied that the model was overfitting.

My first step was to use a convolution neural network model similar to the LeNet with 5 epochs. I thought this model might be appropriate because it is simple and fast algorithm (in order to save my GPU time), but the result was terible. Then I have normlaized the data using Kernas Lambda to normalize the data. It got a bit better but not enough to pass this project.

Then I have decided a bit complexer model `NVIDIA` which is proposed in the course. I have started with small epochs then I have started to increase untill I got lower training loss and validation loss. At some point it started not to change anymore, but I was having overfitting problem, where the training loss was low but validation loss was high. At this point I have used three different camera views, which are right, center and left. Additionally  I have generated augmented data by utilizing flipping method, used Pooling layer with 2x2 pool window size, and Dropout layer with keeping probality '0.5'.

At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road.

#### 2. Final Model Architecture

Here is a visualization of the architecture.

![alt text][image1]

#### 3. Creation of the Training Set & Training Process

I have used the the dataset provided by Udacity since I was having some issue during training mode that I was not able to save my data as soon as I left the GPU mode. Here is the visualization of dataset provided by Udacity.

![alt text][image4]

I loaded the images using the OpenCV `cv2.imread`. For each scene there are three images taken from center, left and right cameras. In oder to be able to use left and right camerea images, I have decided to introduce a correction factor of 0.2, which is also proposed in th course. I have added the correction factor to the steering of the left camera images and I have substracted the correction factor from the steering of the right camere images.

To augment the data sat, I also flipped images and angles thinking that this would hep to increase the number of the data, which results in much better model. For example, here is an image that has then been flipped:

![alt text][image3]


I finally randomly shuffled the data set and put 20% of the data into a validation set. 

Additionaly I have cropped my data to filter out irrelevant information, which can confuse the model during training. The one example of cropped image is given as follows:

![alt text][image2]


I used this training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs for my was 5.  I used an adam optimizer so that manually training the learning rate wasn't necessary. The visialization of means squared error los is as follows: 

![alt text][image5]

The final parametrization of my model is given as follows:

- Number of epochs= 5
- Model optimizer = Adam
- Validation daa split ratio = 0.2
- Generator batch size = 32
- Correction factor =  0.2
- Loss Function Used = MSE

The generated model was able to finish the Track 1 without crossing the border of the way. The recorded video during autonmous simulation is provided .
